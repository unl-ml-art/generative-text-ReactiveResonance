# Machine Learning Project 1: Generative Text

Caleb Kirilov, ckirilov2@huskers.unl.edu

## Abstract

Speech can be a weapon or a tool of silver tongues. Some cause catastrophe, and some spark great awakenings.

Here I have (hopefully) combined some of the most famous/imfluential speeches or excerpts from a wide range of morality. The goal is creating the most impactful speech without a human author.

## Model/Data

The generative text with require several famous speeches from various authors.

-The authors collected for this project in roughly even manner are: Ghandi, MLK, Jesus, Hitler, Idi Amin, Nelson Mandela, Timothy Mcveigh, Carl Sagan, Sadam Hussein.

- GPT2 with fine tuning (an AI best for long term information digestion was the preference)

- The model should be able to interpret scripts of speeches into repeatable patterns with interchangable variables. These variables change the minor aspects while retaining the core speech style throughout. 

## Code (WIP)

Code for training the project:
- ckirilov-gpt2-generate-finetune.ipynb - Generation code

## Results (WIP)

- ckirilov_Speeches_jupyterLab.pdf
- Documents the entire generation and fine tuning process with examples at every 10 step milestone.

## Technical Notes (WIP)

Collect an equal amount of speech data from unique sources, combine into one file, allow 600 steps of GPT2 fine tuning digestion, enter a prefix for your desired speech topic.

## Reference (WIP)

forked from https://github.com/unl-ml-art/generative-text
